{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6223838d-d3f0-4741-8dac-f0f7a0dfadb3",
   "metadata": {},
   "source": [
    "# Image to 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a303fcb9-149c-486f-b887-4f5c6faafaf4",
   "metadata": {},
   "source": [
    "## Transformers Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7e4972",
   "metadata": {},
   "source": [
    "## Render to Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43ff6e-2b40-4284-a9d4-fdf4199bd46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def depth_edges_mask(depth):\n",
    "    \"\"\"Returns a mask of edges in the depth map.\n",
    "    Args:\n",
    "    depth: 2D numpy array of shape (H, W) with dtype float32.\n",
    "    Returns:\n",
    "    mask: 2D numpy array of shape (H, W) with dtype bool.\n",
    "    \"\"\"\n",
    "    # Compute the x and y gradients of the depth map.\n",
    "    depth_dx, depth_dy = np.gradient(depth)\n",
    "    # Compute the gradient magnitude.\n",
    "    depth_grad = np.sqrt(depth_dx**2 + depth_dy**2)\n",
    "    # Compute the edge mask.\n",
    "    mask = depth_grad > 0.01\n",
    "    return mask\n",
    "    # # Save as glb\n",
    "    # glb_file = tempfile.NamedTemporaryFile(suffix='.glb', delete=False)\n",
    "    # glb_path = glb_file.name\n",
    "    # mesh.export(glb_path)\n",
    "    # return glb_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import trimesh\n",
    "import numpy as np\n",
    "# import open3d as o3d\n",
    "from tqdm import tqdm \n",
    "from PIL import Image\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "from pytorch3d.io import load_objs_as_meshes, load_obj\n",
    "\n",
    "import pytorch3d\n",
    "from pytorch3d.structures import Meshes, Pointclouds\n",
    "import pytorch3d.utils\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras,\n",
    "    PointLights,\n",
    "    Materials,\n",
    "    RasterizationSettings,\n",
    "    MeshRenderer,\n",
    "    MeshRasterizer,\n",
    "    HardPhongShader,\n",
    "    TexturesUV,\n",
    "    TexturesVertex,\n",
    "    Textures\n",
    ")\n",
    "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a9b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch3d import renderer\n",
    "from pytorch3d.vis.plotly_vis import plot_scene, AxisArgs\n",
    "from pytorch3d.renderer.camera_utils import join_cameras_as_batch\n",
    "\n",
    "def get_cameras_looking_at_points(camera_locations, image_size, look_at_points=None, focal_length=0.7):\n",
    "    # Extract device from input `camera_locaitons` tensor\n",
    "    device = camera_locations.device\n",
    "\n",
    "    number_of_cameras = camera_locations.shape[0]\n",
    "\n",
    "    # `look_at_points` defaults to the center\n",
    "    if look_at_points is None:\n",
    "        look_at_points = torch.zeros_like(camera_locations)\n",
    "\n",
    "    if not torch.is_tensor(focal_length):\n",
    "        focal_length = torch.tensor([focal_length]*number_of_cameras).to(dtype=torch.float32, device=device)\n",
    "\n",
    "    # Get camera rotation and translation\n",
    "    R, T = pytorch3d.renderer.look_at_view_transform(at=look_at_points, eye=camera_locations)\n",
    "\n",
    "    image_size = image_size.unsqueeze(0).expand(number_of_cameras, -1)\n",
    "\n",
    "    # Define Camera\n",
    "    cameras = renderer.cameras.PerspectiveCameras(\n",
    "        focal_length=focal_length,\n",
    "        principal_point=image_size/2,\n",
    "        R=R,\n",
    "        T=T,\n",
    "        in_ndc=False, \n",
    "        image_size=image_size.flip(-1),\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        cameras[0],\n",
    "        join_cameras_as_batch([cameras[i] for i in range(1,number_of_cameras)]) if number_of_cameras > 1 else None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00bb88a-f47c-4269-9618-23bdb2844eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imsz = torch.tensor(image.size)\n",
    "imsz.unsqueeze(0).expand(3, -1).flip(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809fb06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mesh(obj_file_path, device=\"cpu\"):\n",
    "    mesh = load_objs_as_meshes([obj_file_path], device=device)\n",
    "    verts, faces = mesh.get_mesh_verts_faces(0)\n",
    "    texture_rgb = torch.ones_like(verts, device=device)\n",
    "    texture_rgb[:, 1:] *= 0.0  # red, by zeroing G and B\n",
    "    mesh.textures = Textures(verts_rgb=texture_rgb[None])\n",
    "    \n",
    "    # Normalize mesh\n",
    "    verts = verts - verts.mean(dim=0)\n",
    "    verts /= verts.max()\n",
    "    \n",
    "    # This updates the pytorch3d mesh with the new vertex coordinates.\n",
    "    mesh = mesh.update_padded(verts.unsqueeze(0))\n",
    "    verts, faces = mesh.get_mesh_verts_faces(0)\n",
    "    \n",
    "    return mesh, verts, faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429cb0c-f998-450b-938e-83a77cd47bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_faces_from_grid(H, W, device):\n",
    "    # Source (modified): https://huggingface.co/spaces/shariqfarooq/ZoeDepth/blob/main/gradio_im_to_3d.py\n",
    "    \"\"\"Creates mesh triangle indices from a given pixel grid size.\n",
    "        This function is not and need not be differentiable as triangle indices are\n",
    "        fixed.\n",
    "\n",
    "    Args:\n",
    "        h: (int) denoting the height of the image.\n",
    "        w: (int) denoting the width of the image.\n",
    "\n",
    "    Returns:\n",
    "        triangles: 2D numpy array of indices (int) with shape (2(W-1)(H-1) x 3)\n",
    "    \"\"\"\n",
    "    '''\n",
    "    00---01\n",
    "    |    |\n",
    "    10---11\n",
    "    '''\n",
    "    vertex_ids = torch.arange(H*W).reshape(H, W).to(depth.device)\n",
    "    vertex_00 = vertex_ids[:H-1, :W-1]\n",
    "    vertex_01 = vertex_00 + 1\n",
    "    vertex_10 = vertex_00 + W\n",
    "    vertex_11 = vertex_00 + W + 1\n",
    "\n",
    "    return torch.cat(\n",
    "        torch.stack(\n",
    "            # counter-clockwise orientation\n",
    "            # TODO: does the order matter?\n",
    "            [\n",
    "                vertex_00, vertex_10, vertex_01, # faces_upper_left_triangle\n",
    "                 vertex_10, vertex_11, vertex_01 # faces_lower_right_triangle\n",
    "            ]\n",
    "        ).flatten(1).chunk(2),\n",
    "        dim=1\n",
    "    ).permute(1, 0)\n",
    "\n",
    "\n",
    "def edge_threshold_filter(vertices, faces, edge_threshold=0.1, test=1):\n",
    "    \"\"\"\n",
    "    Only keep faces where all edges are smaller than edge_threshold.\n",
    "    Will remove stretch artifacts that are caused by inconsistent depth at object borders\n",
    "\n",
    "    :param vertices: (N, 3) torch.Tensor of type torch.float32\n",
    "    :param faces: (M, 3) torch.Tensor of type torch.long\n",
    "    :param edge_threshold: maximum length per edge (otherwise removes that face).\n",
    "\n",
    "    :return: filtered faces\n",
    "    \"\"\"\n",
    "    # if test==2:\n",
    "        \n",
    "\n",
    "    edge_distances = torch.linalg.vector_norm(vertices[faces] - vertices[faces].roll(shifts=-1, dims=1), dim=2)\n",
    "    if test==1:\n",
    "        edge_distances = (vertices[faces] - vertices[faces].roll(shifts=-1, dims=1))[:, :, 2]\n",
    "        \n",
    "    mask_small_edge = (edge_distances < edge_threshold).all(1)\n",
    "\n",
    "    return faces[mask_small_edge, :], faces[(edge_distances >= edge_threshold).any(1), :]\n",
    "\n",
    "@torch.inference_mode\n",
    "def predict_depth(image, image_processor, model):\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    outputs_flip = model(pixel_values=torch.flip(inputs.pixel_values, dims=[3]))\n",
    "\n",
    "    return post_process_depth_estimation_zoedepth(outputs, [image.size[::-1]], outputs_flip=outputs_flip)[0][\n",
    "        \"predicted_depth\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f4b20-a43f-4e3b-a0a4-9e5967efe23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor(images=[image, image], return_tensors=\"pt\")[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6666320a-fc67-4319-a9c1-416287330a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_points = {}\n",
    "# for test in [True, False]:\n",
    "#     num_points[test] = {}\n",
    "#     for focal_length in np.arange(0.1, 1.8, 0.1):\n",
    "#         num_points[test][focal_length.item()] = {}\n",
    "#         for threshold in np.arange(0.001, 0.04, 0.001):\n",
    "#             cameras = get_cameras_looking_at_points(torch.tensor(\n",
    "#                 [\n",
    "#                     0.0,\n",
    "#                     0.0,\n",
    "#                     predicted_depth[predicted_depth.shape[0]//2, predicted_depth.shape[1]//2]\n",
    "#                 ]).unsqueeze(0), focal_length=focal_length.item())[0]\n",
    "#             world_points = cameras.unproject_points(xyz)\n",
    "#             num_points[test][focal_length.item()][threshold.item()] = edge_threshold_filter(\n",
    "#                 world_points, faces_new, threshold, test=test\n",
    "#             )[0].shape[0]\n",
    "\n",
    "# import plotly.graph_objects as go\n",
    "# import numpy as np\n",
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "# # Create a figure object\n",
    "# fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "# # Add a surface plot for each combination of test, focal_length, and threshold\n",
    "# for test in [True]:#, False]:\n",
    "#     for threshold in np.arange(0.001, 0.04, 0.001):\n",
    "#         num_points_val = [\n",
    "#             num_points[True][focal_length.item()][threshold.item()]\n",
    "#             for focal_length in np.arange(0.1, 1.8, 0.1)\n",
    "#         ]\n",
    "#         fig.add_trace(\n",
    "#             go.Scatter(\n",
    "#                 x=np.arange(0.1, 1.8, 0.1), y=num_points_val,\n",
    "#                 mode='lines', name=f\"Test=True, Threshold={threshold:.4f}\"\n",
    "#             ),\n",
    "#             row=1, col=1\n",
    "#         )\n",
    "#         num_points_val = [\n",
    "#             num_points[False][focal_length.item()][threshold.item()]\n",
    "#             for focal_length in np.arange(0.1, 1.8, 0.1)\n",
    "#         ]\n",
    "#         fig.add_trace(\n",
    "#             go.Scatter(\n",
    "#                 x=np.arange(0.1, 1.8, 0.1), y=num_points_val,\n",
    "#                 mode='lines', name=f\"Test=False, Threshold={threshold:.4f}\"\n",
    "#             ),\n",
    "#             row=1, col=2\n",
    "#         )\n",
    "\n",
    "# # Set the layout and show the plot\n",
    "# # fig.update_layout(title='Number of Points',\n",
    "# #                    scene=dict(xaxis_title='Test',\n",
    "# #                                yaxis_title='Focal Length',\n",
    "# #                                zaxis_title='Threshold'))\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a57aa1-371d-41c9-a882-3e506b4de9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_coordinates_pt3d(\n",
    "    height: int,\n",
    "    width: int,\n",
    "    device: torch.device = torch.device('cpu')\n",
    "):\n",
    "    \"\"\"For an image with y_resolution and x_resolution, return a tensor of pixel coordinates\n",
    "    normalized to lie in [0, 1], with the origin (0, 0) in the bottom left corner,\n",
    "    the x-axis pointing right, and the y-axis pointing up. The top right corner\n",
    "    being at (1, 1).\n",
    "\n",
    "    Returns:\n",
    "        xy_pix: a meshgrid of values from [0, 1] of shape \n",
    "                (y_resolution, x_resolution, 2)\n",
    "    \"\"\"\n",
    "    xs = torch.arange(width-1, -1, -1)  # Inverted the order for x-coordinates\n",
    "    ys = torch.arange(height-1, -1, -1)  # Inverted the order for y-coordinates\n",
    "    x, y = torch.meshgrid(xs, ys, indexing='xy')\n",
    "\n",
    "    return torch.cat([x.unsqueeze(dim=2), y.unsqueeze(dim=2)], dim=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43094a7-695a-4785-bb44-e75c1a147e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, ZoeDepthForDepthEstimation\n",
    "\n",
    "from zoedepth_post_processing import post_process_depth_estimation_zoedepth\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"Intel/zoedepth-nyu\")\n",
    "model = ZoeDepthForDepthEstimation.from_pretrained(\"Intel/zoedepth-nyu\")\n",
    "\n",
    "# prepare image for the model\n",
    "url = \"https://shariqfarooq-zoedepth.hf.space/file=/home/user/app/examples/person_1.jpeg\"\n",
    "url = \"https://shariqfarooq-zoedepth.hf.space/file=/home/user/app/examples/mountains.jpeg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image.thumbnail((512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bda864-cd57-416a-9d14-b4fca3290ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28085d0-2adb-4df4-ba33-590b3ad398da",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_depth = predict_depth(image, image_processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf04a9-a466-4369-9dd8-d94d386eaab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(((predicted_depth/predicted_depth.max())*255).astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1953be-52a3-41a9-9ee9-72d49bf601a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sph2cart(az, el, r):\n",
    "    # source https://github.com/numpy/numpy/issues/5228#issue-46746558\n",
    "    rcos_theta = r * np.cos(np.radians(el))\n",
    "    x = rcos_theta * np.cos(np.radians(az))\n",
    "    y = rcos_theta * np.sin(np.radians(az))\n",
    "    z = r * np.sin(np.radians(el))\n",
    "    return np.round(x, 5), np.round(y, 5), np.round(z, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf0cc2d-1aaa-4298-b24d-0e6c3c0e5297",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(np.stack(sph2cart([45*i for i in range(8)], [80]*8, [3]*8)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf813be-40b8-4d94-afc9-f7926126874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_0 = predicted_depth[predicted_depth.shape[0]//2, predicted_depth.shape[1]//2]\n",
    "z_0 = (predicted_depth.max() + predicted_depth.min())/2\n",
    "# main_camera, other_cameras = get_cameras_looking_at_points(\n",
    "#     camera_locations = torch.tensor([[0, 0, z_0]] + [\n",
    "#         [x, y, z_0]\n",
    "#         for x in [-z_0, 0, z_0]\n",
    "#         for y in [-z_0, 0, z_0]\n",
    "#         if x+y != x*y\n",
    "#     ]).to(dtype=torch.float32, device=device),\n",
    "#     image_size=torch.tensor(image.size).to(dtype=torch.float32, device=device),\n",
    "#     focal_length=6 * image.size[0]\n",
    "# )\n",
    "device=\"cpu\"\n",
    "main_camera, other_cameras = get_cameras_looking_at_points(\n",
    "    camera_locations = torch.tensor(\n",
    "        [[0, 0, z_0]] + torch.tensor(np.stack(\n",
    "            sph2cart([45*i for i in range(8)], [80]*8, [z_0]*8)\n",
    "        ).T).tolist()\n",
    "    ).to(dtype=torch.float32, device=device),\n",
    "    image_size=torch.tensor(image.size).to(dtype=torch.float32, device=device),\n",
    "    focal_length=0.5 * image.size[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72733a6-dedd-428a-9ff0-2f4a7f9d9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cpu\"\n",
    "img_resolution = image.size[::-1]\n",
    "xy_pix = get_pixel_coordinates_pt3d(img_resolution[0], img_resolution[1], device=device)\n",
    "xy_pix = xy_pix.flatten(0, -2)\n",
    "depth = torch.tensor(predicted_depth).unsqueeze(2).flatten(0, -2)\n",
    "xyz = torch.cat((xy_pix, depth), dim=1)\n",
    "world_points = main_camera.unproject_points(xyz)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bee9b1-408d-4950-a982-6b0b2e59a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d702ef4e-5fd2-484f-a5b0-fcdc0e16ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_new = generate_faces_from_grid(image.size[1], image.size[0], \"cpu\")\n",
    "faces_filtered, faces_removed = edge_threshold_filter(world_points, faces_new, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3dabf9-bfd4-4adb-8c06-64a6fa960ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(faces_removed))\n",
    "print(len(faces_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48d0e9a-5bdd-4961-afd3-6858287df047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.renderer import TexturesVertex\n",
    "\n",
    "colors = torch.tensor(np.array(image).reshape(-1, 3)/255)\n",
    "colors[faces_removed.unique()] = torch.zeros(3).to(colors)\n",
    "textures = TexturesVertex(verts_features=colors.unsqueeze(0))\n",
    "# textures_white = TexturesVertex(verts_features=torch.tensor(np.array(image).reshape(-1, 3)*0.0 + 1).unsqueeze(0))\n",
    "# textures_white = TexturesAtlas(atlas=torch.ones(size=(1, faces_removed.shape[0], 1, 1, 3)))\n",
    "\n",
    "trg_mesh = Meshes(verts=[world_points], faces=[faces_new], textures=textures)\n",
    "# bad_mesh = Meshes(verts=[world_points], faces=[faces_removed], textures=textures_white)\n",
    "verts, faces = trg_mesh.get_mesh_verts_faces(0)\n",
    "print(\n",
    "    f\"\\nVertices: {verts.shape}\"\n",
    "    f\"\\nFaces: {faces.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397ef799-886e-4410-a0a9-4893c67d5c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud = Pointclouds(points=[world_points], features=[colors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aef358-5155-464f-9ecf-bec57dfead71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import inspect\n",
    "inspect(plot_scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc7053-1853-485a-b232-a53388036ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_scene(\n",
    "    {\n",
    "        \"mesh\": {\n",
    "            \"mesh\": trg_mesh,\n",
    "            # \"mesh_bad\": bad_mesh,\n",
    "            \"main_camera\": main_camera,\n",
    "            \"other_cameras\": other_cameras,\n",
    "        },\n",
    "        \"mesh2\": {\n",
    "            \"pointcloud\": point_cloud,\n",
    "            \"main_camera\": main_camera,\n",
    "            \"other_cameras\": other_cameras,\n",
    "        }\n",
    "    },\n",
    "    axis_args=AxisArgs(backgroundcolor=\"rgb(200,230,200)\", showgrid=True, showticklabels=True),\n",
    "    ncols=1,\n",
    "    viewpoint_cameras=main_camera\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=1500,\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5b283-ce44-46a9-af5c-a6655446339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import inspect\n",
    "inspect(trg_mesh, all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98820da4-7315-4b9d-accb-1f103ab529b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4024d8b2-0406-4a6f-b3bd-a5c288a5221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "# Util function for loading meshes\n",
    "from pytorch3d.io import load_objs_as_meshes, load_obj\n",
    "import time\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
    "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVPerspectiveCameras,\n",
    "    OpenGLPerspectiveCameras,\n",
    "    PointLights,\n",
    "    diffuse,\n",
    "    Materials, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    SoftPhongShader,\n",
    "    TexturesAtlas,\n",
    "    BlendParams,\n",
    "    AmbientLights\n",
    ")\n",
    "\n",
    "BlendParams.background_color = (1.,1.,1.)\n",
    "BlendParams.gamma = 0.001\n",
    "\n",
    "# Settings for rasterization\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=(512, 341),\n",
    "    blur_radius=0.0,\n",
    "    faces_per_pixel=1,\n",
    "    max_faces_per_bin = 50000,\n",
    ")\n",
    "\n",
    "# class Lighting(NamedTuple):  # pragma: no cover\n",
    "#     ambient: float = 0.8\n",
    "#     diffuse: float = 1.0\n",
    "#     fresnel: float = 0.0\n",
    "#     specular: float = 0.0\n",
    "#     roughness: float = 0.5\n",
    "#     facenormalsepsilon: float = 1e-6\n",
    "#     vertexnormalsepsilon: float = 1e-12\n",
    "\n",
    "# # Setting the lights\n",
    "lights = PointLights(\n",
    "    ambient_color=((0.7, 0.7, 0.7), ),\n",
    "    diffuse_color=((0.1, 0.1, 0.1), ),\n",
    "    specular_color=((0.1, 0.1, 0.1), ),\n",
    "    device=device,\n",
    "    location=(\n",
    "        torch.cat([other_camera.T for other_camera in other_cameras]).tolist()\n",
    "    )\n",
    ")\n",
    "\n",
    "# Setting the renderer\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=other_cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftPhongShader(\n",
    "        device=device, \n",
    "        cameras=other_cameras,\n",
    "        lights=lights\n",
    "    )\n",
    ")\n",
    "from pytorch3d.structures import join_meshes_as_batch\n",
    "# Rendering the image\n",
    "images = renderer(join_meshes_as_batch([trg_mesh]*len(other_cameras)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541bad0-cdea-4db7-b4d9-6b81670d4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "images_p = images.clone()\n",
    "images_p = (images_p.numpy()*255).astype(\"uint8\")\n",
    "images_p = [Image.fromarray(image_p) for image_p in images_p]\n",
    "images_p = images_p[:4] + [image] + images_p[4:]\n",
    "\n",
    "images_grid = image_grid([images_p[i] for i in [3, 2, 1, 5, 4, 0, 6, 7, 8]], 3, 3)\n",
    "images_grid.save(\"out.png\")\n",
    "images_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4af181-f06b-4122-b0c5-75015eb77803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting the lights\n",
    "lights = PointLights(\n",
    "    device=device,\n",
    "    location=(\n",
    "        torch.cat([main_camera.T]).tolist()\n",
    "    )\n",
    ")\n",
    "\n",
    "# Setting the renderer\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=main_camera, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftPhongShader(\n",
    "        device=device, \n",
    "        cameras=main_camera,\n",
    "        lights=lights\n",
    "    )\n",
    ")\n",
    "from pytorch3d.structures import join_meshes_as_batch\n",
    "# Rendering the image\n",
    "main_render = renderer(trg_mesh)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a6e5e-98a2-4829-82f2-b6c2b6c64135",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray((main_render.numpy()*255).astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc42490-1f52-459d-9af5-e594353cb12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0741e447-3fcf-43a7-b303-6c088e7df9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall diffusers -y\n",
    "!pip install git+https://github.com/huggingface/diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86499b10-6dda-4de6-bcae-6153f771dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers.utils import load_image, check_min_version\n",
    "from diffusers.pipelines import StableDiffusion3ControlNetInpaintingPipeline\n",
    "from diffusers.models.controlnet_sd3 import SD3ControlNetModel\n",
    "\n",
    "controlnet = SD3ControlNetModel.from_pretrained(\n",
    "    \"alimama-creative/SD3-Controlnet-Inpainting\", use_safetensors=True, extra_conditioning_channels=1\n",
    ")\n",
    "pipe = StableDiffusion3ControlNetInpaintingPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3-medium-diffusers\",\n",
    "    controlnet=controlnet,\n",
    "    # torch_dtype=torch.float16,\n",
    ")\n",
    "# pipe.text_encoder.to(torch.float16)\n",
    "# pipe.controlnet.to(torch.float16)\n",
    "# pipe.to(\"cuda\")\n",
    "\n",
    "image = load_image(\n",
    "    \"https://huggingface.co/alimama-creative/SD3-Controlnet-Inpainting/resolve/main/images/dog.png\"\n",
    ")\n",
    "mask = load_image(\n",
    "    \"https://huggingface.co/alimama-creative/SD3-Controlnet-Inpainting/resolve/main/images/dog_mask.png\"\n",
    ")\n",
    "width = 1024\n",
    "height = 1024\n",
    "prompt = \"A cat is sitting next to a puppy.\"\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(24)\n",
    "# res_image = pipe(\n",
    "#     negative_prompt=\"deformed, distorted, disfigured, poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, mutated hands and fingers, disconnected limbs, mutation, mutated, ugly, disgusting, blurry, amputation, NSFW\",\n",
    "#     prompt=prompt,\n",
    "#     height=height,\n",
    "#     width=width,\n",
    "#     control_image=image,\n",
    "#     control_mask=mask,\n",
    "#     num_inference_steps=28,\n",
    "#     generator=generator,\n",
    "#     controlnet_conditioning_scale=0.95,\n",
    "#     guidance_scale=7,\n",
    "# ).images[0]\n",
    "# res_image.save(f\"sd3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ff0f7d-363b-4eba-94e5-8af9b587bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee31a52-33fc-403f-900e-3968932351d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "image = Image.open(\"./out.png\")\n",
    "image\n",
    "mask = Image.fromarray(np.where(\n",
    "    np.logical_or((np.array(image)==255).all(2), (np.array(image)==0).all(2))[:, :, None],\n",
    "    np.ones_like(image)*255,\n",
    "    np.zeros_like(image),\n",
    "))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9c412-8306-4275-92b5-7ff24405ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "import torch\n",
    "\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-inpainting\",\n",
    "    revision=\"fp16\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "prompt = \"realistic, high quality\"\n",
    "#image and mask_image should be PIL images.\n",
    "#The mask structure is white for inpainting and black for keeping as is\n",
    "image = pipe(prompt=prompt, image=image, mask_image=mask).images[0]\n",
    "# image.save(\"./yellow_cat_on_park_bench.png\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51ff7d-f8ce-4e77-8b62-ed458a120d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.crop((0, 0, 314, 512)).save(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c339c-510e-4502-af9e-65476d93c99c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
